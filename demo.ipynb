{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_P300_dataset:(170, 64, 64)\n",
      "train_P300_label:(170, 1)\n",
      "train_non_P300_dataset:(850, 64, 64)\n",
      "train_non_P300_label:(850, 1)\n"
     ]
    }
   ],
   "source": [
    "Subjects = ['A','B']\n",
    "Subject = Subjects[1]\n",
    "samps = 64\n",
    "Target,Target_label,NonTarget,NonTarget_label = train_data_and_label(Subject,samps)\n",
    "Target, Target_label = shuffled_data(Target, Target_label)\n",
    "NonTarget, NonTarget_label = shuffled_data(NonTarget, NonTarget_label)\n",
    "train_P300_dataset = Target\n",
    "train_P300_label = Target_label\n",
    "train_non_P300_dataset = NonTarget\n",
    "train_non_P300_label = NonTarget_label\n",
    "print ('train_P300_dataset:' + str(train_P300_dataset.shape))\n",
    "print ('train_P300_label:' + str(train_P300_label.shape))\n",
    "print ('train_non_P300_dataset:'+ str(train_non_P300_dataset.shape))\n",
    "print ('train_non_P300_label:' + str(train_non_P300_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_P300_dataset:(200, 64, 64)\n",
      "test_P300_label:(200, 1)\n",
      "test_non_P300_dataset:(1000, 64, 64)\n",
      "test_non_P300_label:(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "Target,Target_label,NonTarget,NonTarget_label = test_data_and_label(Subject,samps)\n",
    "Target, Target_label = shuffled_data(Target, Target_label)\n",
    "NonTarget, NonTarget_label = shuffled_data(NonTarget, NonTarget_label)\n",
    "test_P300_dataset = Target\n",
    "test_P300_label = Target_label\n",
    "test_non_P300_dataset = NonTarget\n",
    "test_non_P300_label = NonTarget_label\n",
    "print ('test_P300_dataset:' + str(test_P300_dataset.shape))\n",
    "print ('test_P300_label:' + str(test_P300_label.shape))\n",
    "print ('test_non_P300_dataset:'+ str(test_non_P300_dataset.shape))\n",
    "print ('test_non_P300_label:' + str(test_non_P300_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepocessing(data):\n",
    "    pd_data = np.zeros(data.shape)\n",
    "    for i in range(data.shape[0]):\n",
    "        pd_data[i,:,:] = preprocessing.normalize(data[i,:,:], norm='l2')\n",
    "    return pd_data\n",
    "def add_dimension(x, y):                                                        \n",
    "    x = np.reshape(x,[x.shape[0],x.shape[2],x.shape[1]]).astype('float32')\n",
    "    y = np.reshape(y,[y.shape[0],1]).astype('float32')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (1020, 64, 64)\n",
      "train_label: (1020, 1)\n",
      "test_data: (1200, 64, 64)\n",
      "test_label: (1200, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.vstack((train_P300_dataset,train_non_P300_dataset))\n",
    "train_label = np.vstack((train_P300_label,train_non_P300_label))\n",
    "train_data, train_label = shuffled_data(train_data, train_label)\n",
    "\n",
    "test_data = np.vstack((test_P300_dataset,test_non_P300_dataset))\n",
    "test_label = np.vstack((test_P300_label,test_non_P300_label))\n",
    "test_data, test_label = shuffled_data(test_data, test_label)\n",
    "\n",
    "\n",
    "train_data, train_label = add_dimension(train_data,train_label)\n",
    "test_data, test_label = add_dimension(test_data, test_label)\n",
    "print('train_data:',train_data.shape)\n",
    "print('train_label:',train_label.shape)\n",
    "print('test_data:',test_data.shape)\n",
    "print('test_label:',test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, Data, Label):\n",
    "        self.Data = torch.from_numpy(Data)           \n",
    "        self.Label = torch.from_numpy(Label)\n",
    "    def __getitem__(self, index):\n",
    "        return self.Data[index], self.Label[index]\n",
    "    def __len__(self):\n",
    "        return len(self.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channel = 64\n",
    "input_length = 64\n",
    "output_channel = 1              \n",
    "channel_sizes = [64,64,64,64]                  \n",
    "kernel_size = 7\n",
    "input = torch.randn(1, 64, 64)\n",
    "net = STNN_net(input_channel, input_length,output_channel, channel_sizes,kernel_size)\n",
    "net(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "epochs = 15\n",
    "b_size = 32\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "Dataset = Dataset(train_data,train_label)\n",
    "Train_loader = DataLoader(Dataset,batch_size = b_size ,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,train_data,train_label):\n",
    "    train_loss = 0.0\n",
    "    train_loss_array = []\n",
    "    for step, (inputs, labels) in enumerate(Train_loader): \n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_loss_array.append(loss.item())\n",
    "    return train_loss_array\n",
    "def test(net,epoch,vaild_data,vaild_label):\n",
    "    predicted = []\n",
    "    inputs = torch.from_numpy(vaild_data)\n",
    "    labels = torch.from_numpy(vaild_label)\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    predicted = net(inputs)\n",
    "    predicted = predicted.data.cpu().numpy()\n",
    "    labels = labels.data.numpy()\n",
    "    accuracy_score = skm.accuracy_score(labels, np.round(predicted))\n",
    "    return accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  1\n",
      " accuracy_score 0.8408333333333333\n",
      "\n",
      "Epoch  2\n",
      " accuracy_score 0.8675\n",
      "\n",
      "Epoch  3\n",
      " accuracy_score 0.8683333333333333\n",
      "\n",
      "Epoch  4\n",
      " accuracy_score 0.9016666666666666\n",
      "\n",
      "Epoch  5\n",
      " accuracy_score 0.9033333333333333\n",
      "\n",
      "Epoch  6\n",
      " accuracy_score 0.905\n",
      "\n",
      "Epoch  7\n",
      " accuracy_score 0.88\n",
      "\n",
      "Epoch  8\n",
      " accuracy_score 0.8933333333333333\n",
      "\n",
      "Epoch  9\n",
      " accuracy_score 0.875\n",
      "\n",
      "Epoch  10\n",
      " accuracy_score 0.9133333333333333\n",
      "\n",
      "Epoch  11\n",
      " accuracy_score 0.91\n",
      "\n",
      "Epoch  12\n",
      " accuracy_score 0.9158333333333334\n",
      "\n",
      "Epoch  13\n",
      " accuracy_score 0.9133333333333333\n",
      "\n",
      "Epoch  14\n",
      " accuracy_score 0.9108333333333334\n",
      "\n",
      "Epoch  15\n",
      " accuracy_score 0.9116666666666666\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):  \n",
    "    print (\"\\nEpoch \", epoch)\n",
    "    train_loss=train(epoch,train_data,train_label)\n",
    "    accuracy_score = test(net,epoch,test_data, test_label)\n",
    "    print (' accuracy_score', accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
